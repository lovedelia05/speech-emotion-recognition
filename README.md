# speech-emotion-recognition

# Overview
This project focuses on Speech Emotion Recognition (SER), which aims to identify human emotions from speech signals. By using machine learning and deep learning techniques, the model classifies emotions such as happiness, sadness, anger, and neutrality based on features extracted from audio files.

# Features
- Audio Preprocessing: Including denoising, normalization, and segmentation.
- Feature Extraction: Using methods such as Mel-Frequency Cepstral Coefficients (MFCC), Linear Predictive Coding (LPC), and Short-Time Fourier Transform (STFT).
Modeling: Implemented various machine learning and deep learning models (SVM, CNN, LSTM) for emotion classification.
Data Augmentation: Enhanced model robustness with techniques like noise injection, time stretching, and pitch shifting.
Evaluation Metrics: Performance assessed using accuracy, precision, recall, and F1-score.
# Dataset
The primary dataset used is the RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) dataset, which includes recordings of various actors speaking in different emotional tones. The dataset contains audio files with the following emotions:

Neutral
Calm
Happy
Sad
Angry
Fearful
Disgust
Surprised

# Downloading the Dataset
You can download the RAVDESS dataset from https://zenodo.org/record/1188976 .
